<h1 id="thepromiseofaltmetricsfordevelopingregions">The promise of altmetrics for developing regions</h1>

<p>A significant group of scholars from around the world love to hate the Journal Impact Factor (JIF).[1] An incredible amount of ink (and electrons) have been spilled on describing its methodological limitations, its abuse and misuse, and its pervasive effects on &#8220;science.&#8221; But, while the loathing of the JIF, I hazard to guess, is distributed fairly equally around the world, the scholars affected by its use are not. It is scholars from developing regions, especially those working in the social sciences and humanities, who suffer the most egregious consequences.</p>

<p>The problems for developing regions stem from the under-representation of developing world research in Thomson Reuters&#8217; Web of Science (WoS), from which the JIF is calculated. In a seminal piece from fifteen years ago, Cetto &amp; Alonso-Gamboa (1998). lay out the disheartening situation of Latin American journals in international information systems such as the WoS. As can be seen from the figure below, which shows the relative number of works authored by scholars from around the world, this situation has not significantly changed over time. </p>

<p>To be clear, it is not that developing regions simply produce less research. In Latin America, to draw on the region I am most familiar with, there were only 242 journals in WoS in 2012, out of over <a href="http://www.latindex.org/index.html?opcion=2">5,000 peer-reviewed journals</a> published in Latin America (see <a href="http://www.latindex.org/">Latindex</a>). Two initiatives, <a href="http://www.scielo.org">SciELO</a> and <a href="http://www.redalyc.org">RedALyC</a>, working with only a subset of these journals, have also indexed hundreds of thousands of articles in regional journals, primarily from Latin American authors.[2] This is only to say, the research is there—locally produced and published in established venues. </p>

<p>The argument for this bias in WoS has always been that developing world research does not form part of &#8220;mainstream&#8221; or &#8220;international&#8221; science, and although I take exception to this argument, it is irrelevant here. The end result, regardless of its rationale, is that the WoS is an inadequate dataset to understand the impact or otherwise study scholarly communications from developing regions. To serve scholars from developing regions, it is imperative to find an alternative.</p>

<p><iframe src="http://jalperin.github.io/d3-cartogram/"></iframe></p>

<h2 id="analternativetothejournalimpactfactor">An alternative to the Journal Impact Factor</h2>

<p>The scholarly community is abuzz with <em><a href="http://en.wikipedia.org/wiki/Altmetrics">altmetrics</a></em> and the related (but different) term <em>Article Level Metrics</em> (see Figure 2, Mulvani, 2014). These metrics, derived primarily from &#8220;the social Web,&#8221; have been purposely constructed to be alternatives to the JIF. Since the drafting of the <a href="http://altmetrics.org/manifesto/">altmetrics manifesto</a>, there has been a <a href="http://asis.org/Bulletin/Apr-13/">special issue</a>, a <a href="http://www.ploscollections.org/altmetrics">PLOS collection</a>, a <a href="http://www.mendeley.com/groups/586171/altmetrics/">Mendeley group</a>, several <a href="http://lanyrd.com/2013/alm13/">annual</a> <a href="http://altmetrics.org/altmetrics14/">workshops</a>, an increasing number of research papers, and several start-ups. During a few months in the last 12 months, the term <em>altmetrics</em> has even overtaken the terms <em>bibliometrics</em> and <em>bibliometric</em> (although the term <em>Journal Impact Factor</em> still dwarfs both). </p>

<p>
    <img src="interest_altmetrics_over_time.png" /><br />
    Or see what this looks like <a href="http://www.google.com/trends/explore?hl=en-US&q=altmetrics,+bibliometrics,+bibliometric&date=today+12-m&cmpt=q&content=1">today</a>
</p>

<p>All of these signs indicate that altmetrics may not remain <em>alternative</em> for long. Whether they supplant or complement the JIF, they bring with them a series of promises, but also some perils, for developing regions. </p>

<h2 id="thepromise">The promise</h2>

<p>Altmetrics are captured from the Web (i.e., social media, blogs, wikipedia), and thus are (somewhat) more democratic. One reader, one vote (technically, one reader, several potential votes). Unlike citations, which are only counted if they come from a select group of journals, altmetrics are counted regardless of where they are originated, with one important consequences: they open the possibility of tracking use in new segments, both within and beyond the academy. </p>

<p>The corollary to this consequence is that scholars can now be rewarded for impact in these new segments. Given that the JIF was only useful for rewarding impact in other journals included in the WoS, and that developing world scholars were systematically under-represented, this means that scholars can now be focus on problems of local and regional interest (be they of interest to academics, or interests to the public). This is of special importance to the social sciences. This is the true promise of altmetrics for the developing world: an opportunity to redirect incentive structures towards problems that contribute to <em>development</em>, or at least to local priorities, be it through academic, policy, personal, or professional-practice impact. </p>

<h2 id="theperil">The peril</h2>

<p>The promise of altmetric is by no means guaranteed. I see two reasons that would keep altmetrics from fulfulling this promise. First, if the altmetrics community does not focus on further understanding the <em>ways</em> in which altmetrics are different from citations. So far, much of the research has found that these new metrics capture a different “dimension”, “flavour”, or “type” of impact than citations, but has not yet gone further (Torres-Salinas et al., 2013; Costas et al., 2014; Haustein &amp; Peters, 2013; Eysenbach, 2011). There is reason to be optimistic—most altmetric research ends with a call for further study of the reliability, validity, and context of the available metrics. Without this understanding, however, altmetrics will only be used as a proxy for traditional citation impact. </p>

<p>Second, if altmetrics end up primarily used as a way of ranking scholars. If this happens, then it will once again turn attention of developing world scholars to the communities in the United States and Europe—for this is where social media has most deeply penetrated. At the <a href="http://lanyrd.com/2013/alm13/">ALM workshop</a> in October 2013, I presented a <a href="https://speakerdeck.com/jalperin/altmetrics-propagating-global-inequality">series of maps</a> showing the levels of penetration of Internet, Twitter, Facebook, and Mendeley (common altmetric sources), and a new map from the Oxford Internet Institute shows the <a href="http://geography.oii.ox.ac.uk/#the-geographically-uneven-coverage-of-wikipedia">uneven geography of Wikipedia</a>. If the name of the game is to increase altmetric scores, it will still be a better strategy to cater to scholars in places where the sources of altmetrics are more heavily used (read: not in the developing world).</p>

<h2 id="howtomoveforward">How to move forward</h2>

<p>I would add here that the field also needs to move away from In most cases, the journals studied are well-known and well-established journals (i.e., Science and Nature) published in so-called “developed” countries (i.e., the US or Western Europe).</p>

<p>One of the greatest accomplishments of the <a href="http://pkp.sfu.ca">Public Knowledge Project</a>, for which I have worked for the past seven years, has been to provide a tool (OJS) that much of the developing world has used to keep journal publishing under local control, free of commercial publishing interests (see our <a href="http://pkp.sfu.ca/ojs/ojs-usage/ojs-map/">OJS Map</a>). </p>

<ul>
<li>focus on building communities

<ul>
<li>when we put out a call for proposals, we learned of 40 like-minded groups</li>
</ul></li>
<li>our approach has been one of shifting research cultures

<ul>
<li>we have done this through OJS</li>
</ul></li>
<li>OJS has allowed the developing world to continue to publish free of commercial interests</li>
<li>need to build tools that:

<ul>
<li>allow scholars to capture metrics on any of their outputs (not just papers)</li>
<li>foster community building, not only research assessment</li>
<li>the manifesto speaks of &#8220;filtering&#8221; (what does it say on discovery)</li>
<li>connecting with our target audience</li>
</ul></li>
<li>Recent NYTimes piece about &#8220;public intellectuals&#8221; (find also responses)</li>
</ul>

<h3 id="references">References</h3>

<p>Cetto, A. M., &amp; Alonso-Gamboa, J. O. (1998). Scientific periodicals in Latin America and the Caribbean: a global perspective. Interciencia, 23(525), 84–93. Retrieved from http://www.interciencia.org/v23_02/cetto.pdf</p>

<p>Costas, R., Zahedi, Z., &amp; Wouters, P. (2014). Do altmetrics correlate with citations? Extensive comparison of altmetric indicators with citations from a multidisciplinary perspective. Digital Libraries. Retrieved from http://arxiv.org/abs/1401.4321</p>

<p>Eysenbach, G. (2011). Can tweets predict citations? Metrics of social impact based on Twitter and correlation with traditional metrics of scientific impact. Journal of Medical Internet Research, 13(4), e123. doi:10.2196/jmir.2012</p>

<p>Haustein, S., &amp; Peters, I. (2013). Tweeting biomedicine: an analysis of tweets and citations in the biomedical literature. Journal of the Association for Information Science and Technology, 1–22. doi:10.1002/asi.23101</p>

<p>Liu, J., &amp; Adie, E. (2013, May 30). Altmetric: Getting Started with Article-Level Metrics. figshare. doi:10.6084/m9.figshare.709018</p>

<p>Mulvany, I. (2014, January 19). Venn diagram of how I see the relationship between altmetrics, article level metrics and journal level metrics http://t.co/kGZJ4LKu0Y [Tweet]. Retrieved from https://twitter.com/IanMulvany/status/424904870643384320</p>

<p>Torres-Salinas, D., Cabezas-Clavijo, Á., &amp; Jiménez-Contreras, E. (2013). Altmetrics: New Indicators for Scientific Communication in Web 2.0. Comunicar, 21(41), 53–60. doi:10.3916/C41&#8211;2013&#8211;05</p>

<p>Wouters, P., &amp; Costas, R. (2012). Users, Narcissism and Control: Tracking the Impact of Scholarly Publications in the 21st Century. Retrieved from http://sticonference.org/Proceedings/vol2/Wouters_Users_847.pdf</p>

<h3 id="footnotes">Footnotes</h3>

<p>[1] The <a href="http://am.ascb.org/dora/">San Francisco Declaration on Research Assessment</a> (DORA), signed by over 10,000 individuals and 400 institutions, provides some of the background and justifications given by scholars for abandonning the JIF. Although it has been signed primarily by Europeans and North Americans, I believe it accurately captures the views of many scholars from around the world who are subjected to the tyranny of the JIF when being assessed. </p>

<p>[2] Thomson Reuters recently announced a partnership with SciELO, whereby journals in SciELO will be indexed and appear in the Web of Knowledge. SciELO also calculates an Impact Factor based on its collection of over 1100 journals. However, even with SciELO, only a fraction of Latin America&#8217;s research can receive an Impact Factor.</p>
